{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3365c8f5",
   "metadata": {},
   "source": [
    "This file countains all the relevant functions to take a body of tweets and transfer them all in their vectorized form into a Mongo Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33b028ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfa1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mongo/Insertion/Retrieval helper functions\n",
    "def matrixToDict(vocab,asList):\n",
    "    tempDict={}\n",
    "    for i in vocab.keys():\n",
    "        innerDict={}\n",
    "        index1 = int(vocab.get(i))\n",
    "        for j in vocab.keys():\n",
    "            index2=int(vocab.get(j))\n",
    "            innerDict[j]=asList[index1][index2]\n",
    "        tempDict[i]=innerDict\n",
    "    return tempDict\n",
    "\n",
    "#retrieveWordsIDs gets current words in the DB and snags their IDs as well\n",
    "def retrieveWordsIDs(db,collectionName):\n",
    "    wordDict={}\n",
    "    cursor = db[collectionName].find({},{'word':1})\n",
    "    for i in cursor:\n",
    "        wordDict[i['word']]=i['_id']\n",
    "    return wordDict\n",
    "\n",
    "def createMatrix(docs):\n",
    "    count_model = CountVectorizer(ngram_range=(1,1),stop_words='english') # default unigram model\n",
    "    X = count_model.fit_transform(docs)\n",
    "    Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "    Xc.setdiag(0) #want to fill same word cooccurence to 0\n",
    "    dense = Xc.todense()\n",
    "    vocab = count_model.vocabulary_\n",
    "    return dense,vocab\n",
    "\n",
    "def insertUpdate(insertList,db,collectionName):\n",
    "    wordidDict = retrieveWordsIDs(db,collectionName)\n",
    "    for i in insertList:\n",
    "        if i['word'] in wordidDict.keys():\n",
    "            cursor = db[collectionName].find({'word':i['word']})\n",
    "            listCursor = list(cursor)\n",
    "            if len(listCursor)!=1:\n",
    "                return None #may as well give up because this should never happen\n",
    "            currentDict = listCursor[0]\n",
    "            currentCounts=currentDict['counts']\n",
    "            currentKeys = list(currentCounts.keys())\n",
    "            #updateKeys = list(set(currentKeys)-set(i['counts'].keys()))\n",
    "            #i think updateKeys not needed\n",
    "            for key in list(set(currentKeys) & set(i['counts'].keys())):\n",
    "                #looping over intersection of keys\n",
    "                currentCounts[key]+=i['counts'][key]\n",
    "            newKeys = list(set(i['counts'].keys())-set(currentKeys))\n",
    "            #taking advantage of set subtraction above\n",
    "            for j in newKeys:\n",
    "                currentCounts[j]=i['counts'][j]\n",
    "            db[collectionName].find_one_and_update({'_id':currentDict['_id']},{\"$set\": {'counts':currentCounts}})\n",
    "        elif i['word'] not in wordidDict.keys():\n",
    "            #just insert\n",
    "            insertDict={}\n",
    "            insertDict['word']=i['word']\n",
    "            insertDict['counts']=i['counts']\n",
    "            db[collectionName].insert(insertDict)\n",
    "    return \"Maybe it did it?\"\n",
    "    #TO-DO; make better return value lol\n",
    "    \n",
    "def findTopN(word,db,collectionName,n=3):\n",
    "    cursor = db[collectionName].find({'word':word})\n",
    "    listCursor = list(cursor)\n",
    "    if len(listCursor)!=1:\n",
    "        return None #may as well give up\n",
    "    counts = listCursor[0]['counts']\n",
    "    returnDict = dict(sorted(counts.items(), key = itemgetter(1), reverse = True)[:n])\n",
    "    return list(returnDict.keys())\n",
    "\n",
    "def fullProcess(doc,db,collName):\n",
    "    dense,vocab = createMatrix(doc)\n",
    "    matrixList = dense.tolist()\n",
    "    wordDict = matrixToDict(vocab,matrixList)\n",
    "    wordList = []\n",
    "    for i in wordDict:\n",
    "        wordList.append({'word':i,'counts':wordDict[i]})\n",
    "    insertUpdate(wordList,db,collName)\n",
    "    return \"good job diego!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab9e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#below shamelessly stolen from lecture\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.functions import udf\n",
    "import string\n",
    "\n",
    "#Create the function that performs the text conversion\n",
    "sn = SnowballStemmer('english')\n",
    "\n",
    "def clean_text(text, sn=sn):\n",
    "    #special case of getting rid of RT info\n",
    "    rt_re = re.compile('^RT @[a-zA-Z0-9]+:')\n",
    "    text = rt_re.sub(' ',text)\n",
    "    #removing other @ instances\n",
    "    ats_re = re.compile('^@[a-zA-Z0-9]+')\n",
    "    text = ats_re.sub(' ',text)\n",
    "    #remove emojis\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    #removing punctuation\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation + '£'))\n",
    "    text = punc_re.sub(' ', ' '+text.lower()+' ') # Pad with spaces for easier stopword removal\n",
    "    # Remove numbers\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    text = num_re.sub(' ', text)\n",
    "    # Remove alphanumerical words\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    text = alpha_num_re.sub(' ', text)\n",
    "    # Stemming\n",
    "    text = sn.stem(text)\n",
    "    # Regex for multiple spaces\n",
    "    spaces_re = re.compile('\\s+')\n",
    "    text = spaces_re.sub(' ', text.strip())\n",
    "\n",
    "    return text\n",
    "\n",
    "clean_text = udf(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd81bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import string\n",
    "import re\n",
    "#no real reason to do this in spark in particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b2c929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36075e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handle,is_rt,date,tweet,qt,rt\r\n",
      "fredjcollins,True,2021-05-27 22:32:23,RT @Ordinary1World: A commission to investigate the attack on the capital should already be in place  but thanks to republican senators it…,,A commission to investigate the attack on the capital should already be in place  but thanks to republican senators it will never happen.  All of those fuckers need to be voted out. Every single one of them.\r\n",
      "Cat0524,True,2021-05-27 22:32:23,RT @scotshelagh: @FreeThinker2030 @Tam__Jardine @BBCScotlandNews @STVNews @BBCNews @itvnews Interesting that they weren’t concerned about U…,,@FreeThinker2030 @Tam__Jardine @BBCScotlandNews @STVNews @BBCNews @itvnews Interesting that they weren’t concerned about U.K. leaving the EU. They didn’t poke their nose into that referendum.  Anyway us Scots are no daft.  Gordon Brown is yesterday’s man and we can see right through him.   Prince William is being used as a puppet of the state.\r\n",
      "VindicatedHobo,False,2021-05-27 22:32:23,@DylanTop5 @SerengetiSover1 It's not funny u bum  it's of utmost importance,,\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 4 data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bdc6168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-------------------+--------------------+----+--------------------+\n",
      "|        handle|is_rt|               date|               tweet|  qt|                  rt|\n",
      "+--------------+-----+-------------------+--------------------+----+--------------------+\n",
      "|  fredjcollins| true|2021-05-27 22:32:23|RT @Ordinary1Worl...|null|A commission to i...|\n",
      "|       Cat0524| true|2021-05-27 22:32:23|RT @scotshelagh: ...|null|@FreeThinker2030 ...|\n",
      "|VindicatedHobo|false|2021-05-27 22:32:23|@DylanTop5 @Seren...|null|                null|\n",
      "+--------------+-----+-------------------+--------------------+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading from file just for simplicities sake\n",
    "data = spark.read.csv('data.csv',\n",
    "                     sep=',',\n",
    "                     header=True,\n",
    "                     inferSchema=True)\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9965abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.registerTempTable('data')\n",
    "nonRTs = spark.sql(r\"\"\"SELECT tweet\n",
    "                            FROM data\n",
    "                            WHERE is_rt='false'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b1591c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|@DylanTop5 @Seren...|\n",
      "|@CNN IN THE BEGIN...|\n",
      "|This stuff with B...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonRTs.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6849e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text pre-processing\n",
    "nonRTs = nonRTs.withColumn('cleanTweet',clean_text(nonRTs['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a7b32fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               tweet|          cleanTweet|\n",
      "+--------------------+--------------------+\n",
      "|@DylanTop5 @Seren...|serengetisover it...|\n",
      "|@CNN IN THE BEGIN...|in the beginning ...|\n",
      "|This stuff with B...|this stuff with b...|\n",
      "|@janrobinjackson ...|cherylej aoc righ...|\n",
      "|These words you s...|these words you s...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonRTs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c90de771",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullList = [i.cleanTweet for i in nonRTs.select('cleanTweet').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d09e5af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1812"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fullList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db= client.testing\n",
    "collName = 'processTest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2179d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ad163f59a1a6>:57: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  db[collectionName].insert(insertDict)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'good job diego!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullProcess(fullList,db,collName)\n",
    "#fullProcess does the CountVectorizing as well as insertion and updating in Mongo\n",
    "#return value is just for encouragement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482c576",
   "metadata": {},
   "source": [
    "Run time on that was a little less than 2 minutes- not ideal, frankly!\n",
    "Generally a better idea to throw in chunks of maybe 500 or so, which runs quite quick in comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7766282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
