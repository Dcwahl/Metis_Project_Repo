{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18671d16",
   "metadata": {},
   "source": [
    "Tweet Intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a700c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class override- apparently this is SOP for tweepy\n",
    "class Listener(tweepy.StreamListener):\n",
    "    \n",
    "    def on_status(self,status):\n",
    "        is_retweet = False\n",
    "        #rt flag, start with false as default\n",
    "        retweet_text= \"\"\n",
    "        if hasattr(status, \"retweeted_status\"):\n",
    "            is_retweet=True\n",
    "            try:\n",
    "                retweet_text = status.retweeted_status.extended_tweet[\"full_text\"]\n",
    "            except:\n",
    "                retweet_text = status.retweeted_status.text\n",
    "        \n",
    "        #handles 140+ character tweets\n",
    "        if hasattr(status, \"extended_tweet\"):\n",
    "            text = status.extended_tweet[\"full_text\"]\n",
    "        else:\n",
    "            text = status.text\n",
    "        \n",
    "        \n",
    "        quoted_text =\"\"\n",
    "        if hasattr(status, \"quoted_status\"):\n",
    "            #check if the QT was 140+ char\n",
    "            if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "                quoted_text = status.quoted_status.extended_tweet['full_text']\n",
    "            else:\n",
    "                quoted_text = status.quoted_status.text\n",
    "                \n",
    "        #some minor text-pre-processing to remove newlines and commas incase those become relevant\n",
    "        remove = [',','\\n']\n",
    "        for i in remove:\n",
    "            text = text.replace(i,\" \")\n",
    "            quoted_text = quoted_text.replace(i,\" \")\n",
    "            retweet_text = retweet_text.replace(i,\" \")\n",
    "            \n",
    "        with open(\"nlpdata.csv\",\"a\",encoding='utf-8') as f:\n",
    "             f.write(\"%s,%s,%s,%s,%s,%s\\n\"%(status.user.screen_name,is_retweet,status.created_at,text,quoted_text,retweet_text))\n",
    "        #db.processTest.insert({'handle':status.user.screen_name,'RT TF':is_retweet,'time':status.created_at,'tweet':text,'qt':quoted_text,'rt':retweet_text,'in_keyword_db':0})\n",
    "    def on_error(self,status_code):\n",
    "        print(\"Encountered streaming error (\", status_code, \")\")\n",
    "        sys.exit()\n",
    "    \n",
    "    #def on_data(self,data):\n",
    "        #print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamListener = Listener()\n",
    "stream = tweepy.Stream(auth=api.auth, listener=streamListener, tweet_mode='extended')\n",
    "with open('nlpdata.csv',\"w\",encoding='utf-8') as f:\n",
    "    f.write(\"handle,is_rt,date,tweet,qt,rt\")\n",
    "tags = [\"biden\",'joe biden'] \n",
    "stream.filter(languages=[\"en\"], track=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f05a8a",
   "metadata": {},
   "source": [
    "Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88821621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "#Create the function that performs the text conversion\n",
    "sn = SnowballStemmer('english')\n",
    "\n",
    "def clean_text(text, sn=sn):\n",
    "    #sort of handles links, not 100% of the time, though\n",
    "    link_re = re.compile('(?:(?:https?|ftp|file):\\/\\/|www\\.|ftp\\.)(?:\\([-A-Z0-9+&@#\\/%=~_|$?!:,.]*\\)|[-A-Z0-9+&@#\\/%=~_|$?!:,.])*(?:\\([-A-Z0-9+&@#\\/%=~_|$?!:,.]*\\)|[A-Z0-9+&@#\\/%=~_|$])')\n",
    "    text=link_re.sub(' ',text)\n",
    "    #special case of getting rid of RT info\n",
    "    rt_re = re.compile('^RT @[a-zA-Z0-9]+:')\n",
    "    text = rt_re.sub(' ',text)\n",
    "    #removing other @ instances\n",
    "    ats_re = re.compile('^@[a-zA-Z0-9]+')\n",
    "    text = ats_re.sub(' ',text)\n",
    "    #remove emojis\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    #removing punctuation\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation + 'Â£'))\n",
    "    text = punc_re.sub(' ', ' '+text.lower()+' ') # Pad with spaces for easier stopword removal\n",
    "    # Remove numbers\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    text = num_re.sub(' ', text)\n",
    "    # Remove alphanumerical words\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    text = alpha_num_re.sub(' ', text)\n",
    "    # Stemming\n",
    "    text = sn.stem(text)\n",
    "    # Regex for multiple spaces\n",
    "    spaces_re = re.compile('\\s+')\n",
    "    text = spaces_re.sub(' ', text.strip())\n",
    "\n",
    "    return text\n",
    "\n",
    "def removeSpecials(text):\n",
    "    #for removal of special case strings\n",
    "    hill_re = re.compile('thehill[a-zA-Z]*')\n",
    "    text = hill_re.sub(' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756b5ab",
   "metadata": {},
   "source": [
    "WordCloud function. Takes in the corpus as a series and outputs wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b413d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(wordcloud_series):\n",
    "    df_text = ' '.join(str(wordcloud_series).lower() for i in wordcloud_series)\n",
    "    wordcloud = WordCloud(stopwords = None,\n",
    "                      collocations=True).generate(df_text)\n",
    "    plt.imshow(wordcloud, interpolation='bilInear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "create_wordcloud(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c506dfa1",
   "metadata": {},
   "source": [
    "VADER 'scoring':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f52b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "summed=0\n",
    "count=0\n",
    "for sentence in tweets.tolist():\n",
    "    vs = analyzer.polarity_scores(str(sentence))\n",
    "    #print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "    summed+=vs['compound']\n",
    "    count+=1\n",
    "print(summed)\n",
    "print(summed/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25cf931",
   "metadata": {},
   "source": [
    "For the VADER scoring, basically intook the Trump and Biden sets separately to get a score on sentiment towards those topics individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd05b7",
   "metadata": {},
   "source": [
    "Example CorEx topic modeling workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=20000,\n",
    "                             stop_words='english',\n",
    "                             binary=True)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(tweets)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "topic_model = ct.Corex(n_hidden=15, words=words, seed=1)\n",
    "topic_model.fit(doc_word, words=words, docs=tweets)\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))\n",
    "#get 'baseline' topics, refine from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c4b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.tc\n",
    "#snag TC of topics\n",
    "#from here workflow is to identify coherent topics from those created by the 'baseline' and use them as anchor words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff77489",
   "metadata": {},
   "source": [
    "Lastly, Scattertext creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41354825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"biden/bidenComp2.csv\",error_bad_lines=False,quoting=3)\n",
    "df2 = pd.read_csv(\"trump/trumpComp2.csv\",error_bad_lines=False)\n",
    "df1['label']='biden' #label the two sets separately\n",
    "df2['label']='trump'\n",
    "tweets=df1[['tweet','label']]\n",
    "tweets2=df2[['tweet','label']]\n",
    "full = pd.concat([tweets,tweets2],ignore_index=True)\n",
    "full['tweet'] =full['tweet'].apply(str)\n",
    "full['tweet']=full['tweet'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41415bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = st.CorpusFromPandas(full,\n",
    "                             category_col='label',\n",
    "                             text_col='tweet',\n",
    "                             nlp=st.whitespace_nlp_with_sentences\n",
    "                            ).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(\n",
    "        corpus,\n",
    "        category='biden',\n",
    "        category_name='Biden',\n",
    "        not_category_name='Trump',\n",
    "        minimum_term_frequency=10,\n",
    "        pmi_threshold_coefficient=5,\n",
    "        width_in_pixels=1000,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870252f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "open('nlpScattertext.html', 'wb').write(html.encode('utf-8'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f4c1f",
   "metadata": {},
   "source": [
    "Of particular note on the scattertext- it seems to fall apart pretty quickly once you get above ~50,000 documents or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee8e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
